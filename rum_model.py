from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# === –ü—É—Ç–∏ ===
MODEL_DIR = "gpt2-lora-sektor-gaza"  # –ø—É—Ç—å –∫ —Ç–≤–æ–µ–π –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

# === –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ ===
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    "ai-forever/rugpt3medium_based_on_gpt2",
    device_map="auto",
    load_in_8bit=True  # –µ—Å–ª–∏ –æ–±—É—á–∞–ª –≤ 8-–±–∏—Ç–∞—Ö
)

model = PeftModel.from_pretrained(base_model, MODEL_DIR)
model.eval()
model.to(device)

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ===
def generate(prompt, max_new_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    output = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        top_k=100,
        top_p=0.95,
        temperature=1.1,
        eos_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# === –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ ===
if __name__ == "__main__":
    while True:
        prompt = input("\n–í–≤–µ–¥–∏ –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç—Ä–æ–∫—É —Ç–µ–∫—Å—Ç–∞ ('exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
        if prompt.lower() == "exit":
            break
        result = generate(prompt)
        print("\nüìú –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(result)
