# ML_laboratory_3
Лабораторная работа №3. Проектирование систем интеллектуального анализа промышленных данных.

# Теоретическая часть

## 1 LLM

  Большие языковые модели (Large Language Models, LLM) — один из типов искусственного интеллекта, который «понимает», обрабатывает и генерирует человеческий язык. Такие возможности он приобретает благодаря глубокому машинному обучению. LLM обучают на огромных массивах данных — текстах, статьях на сайтах, книгах, чтобы модели могли разбираться во множестве тем и тонкостях языка.
  В отличие от первых двух понятий, большие языковые модели обучаются преимущественно для работы с текстами, чтобы помогать человеку в создании полезного уникального контента.

  LLMs / LLM модель (Large Language Model, большая языковая модель) — нейронная лингвистическая сеть, обученная на огромных корпусах данных для понимания и обработки текста.     Искусственный интеллект умеет:
 - переводить тексты — к примеру, с английского на русский и наоборот;
 - писать тексты, статьи, доклады, посты в блог, описания товаров;
 - делать выжимки из материалов — докладов, научных работ, отчётов;
 - поддерживать диалог и отвечать на вопросы и конкретные требования пользователя.

  Большие языковые модели применяются для автоматизации и улучшения процессов в различных направлениях жизни: промышленности, бизнесе, искусстве, медицине. Расскажем подробнее, чем они могут помочь людям и что умеют делать.
  Генерировать тексты и контент. Программы на базе больших языковых моделей анализируют стиль, смысл и содержание и создают контент, на который у человека ушло бы много времени и усилий.
  Например, для digital-агентства генеративная модель GigaChat за три секунды может создать продающий контент любой сложности по указанным характеристикам. При этом описание товаров и услуг для сайта нейронная сеть сделает сразу с SEO-оптимизацией.
  Делать проще взаимодействие с клиентами. На основе LLM создаются чат-боты, которые отвечают клиентам на вопросы о товаре или услуге, вычисляя намерения пользователя. Такие программы рассказывают о характеристиках и преимуществах продукта в режиме реального времени. С их помощью можно получить контакт потенциального покупателя и даже проводить продажи. Использование чат-ботов позволяет уменьшить затраты на обслуживание клиентов на 80%.
  Выполнять функции виртуальных помощников. Виртуальные ассистенты на базе LLM обрабатывают запросы пользователя и помогают решать повседневные разнообразные задачи, например, организацию дел. Их главная сила — умение работать с расплывчатыми и нечёткими запросами.
  Сокращать длинные тексты до резюме. Чат-боты на основе LLM вычленяют главное из текста и делают понятные выжимки. Людям, для которых это важно (научным работникам, менеджерам), не нужно перечитывать 100 страниц текста, чтобы понять суть. Им можно лишь поместить скрипт в чат-бота — и получить качественный материал в виде текста или таблицы.
  Создавать интерактивные обучающие программы. Отдельного внимания заслуживает потенциал LLM в образовании: ИИ генерирует учебные материалы и системы, которые в реальном времени помогают студентам лучше усваивать предмет.
  Помогать со здоровьем. В сфере здравоохранения продвинутые алгоритмы Large Language Models используются для создания виртуальных диагностов, которые помогают пациентам находить связные ответы на вопросы и следить за своим здоровьем. А докторам — проводить анализ данных из истории болезней людей и ставить предварительные диагнозы.
  Переводить тексты с множества языков. При переводе программы LLM учитывают специфику текста, терминологию, стиль, интонацию, пунктуацию. Полученные тексты иногда превосходят те, над которыми работал профессиональный переводчик. А ещё — одна модель часто знает больше языков, чем один человек.
  LLM могут автоматически исправлять ошибки и предлагать варианты улучшения текста. Это особенно полезно для авторов, редакторов и переводчиков, работающих с большими объёмами текстов.
  Проводить расширенный интеллектуальный поиск. LLM эффективно обрабатывает информацию из интернета, используя смысловые запросы вместо просто ключевых слов.

  Принцип работы больших языковых моделей

  Чтобы ИИ распознавал запрос и интент пользователя, а затем генерировал ответ, нужно обучить нейросеть с использованием Machine Learning, NLP Modeling и других.
  Чтобы создать LLM, необходимо:
 1 Собрать много качественных, общих и специфичных данных (поиск, сбор, очистка датасета и т. д.).
 2 Выбрать архитектуру (Transformer, BERT — Bidirectional Encoder Representations from Transformers, GPT — Generative Pre-trained Transformer, T5).
 3 Отточить процесс обучения языковой модели. Масштабировать систему, продумать отладку при сбоях (к примеру, для работы нужно более 1000 видеокарт, есть риск выхода из строя).
 4 Усовершенствовать работу (CUDA-отладчик, библиотека NCCL, Garbage Collectors, фреймворк PyTorch FSDP).
 5 Получить LLM (LL).

---

# Описание разработанной системы

## 1 Lora
  Метод LoRA (Low-Rank Adaptation) был разработан в 2021 году и представлен в данной статье. Его создатели были вдохновлены данной научной работой, в которой авторы отмечают, что, хотя LLM имеют миллионы или даже миллиарды параметров, они имеют низкую "внутреннюю размерность" (intrinsic dimension) при адаптации к новой задаче. Проще говоря, большинство параметров являются избыточными. Из чего можно сделать вывод, что матрицы можно представить пространством меньшей размерности, сохраняя при этом большую часть важной информации.
  Создатели LoRA предположили, что изменение весов при файнтюнинге модели имеет низкий "внутренний ранг" (intrinsic rank). Идея данного метода заключается в том, что для предварительно обученной матрицы весов мы представляем её обновление двумя меньшими матрицами, полученными путем низкоранговой аппроксимации. Эти матрицы мы тренируем при обучении, а исходную матрицу весов замораживаем. Затем для получения окончательного результата мы объединяем исходные и обученные веса.

<div align="center">
  <img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133_trl_peft/lora-animated.gif">
   <p>Иллюстрация работы LoRA Источник: https://huggingface.co/blog/trl-peft</p>
</div>

Одним из важнейших параметров является параметр "r" (ранг). Он определяет размер матриц низкого ранга. При правильном выборе ранга данный метод может показать впечатляющие результаты.

Как работает LoRA
Традиционная тонкая настройка подразумевает обновление всех параметров (или весов модели) предварительно обученной модели на основе новых данных. Для моделей с миллиардами параметров, таких как многие современные LLM или большие модели зрения, этот процесс требует значительных вычислительных ресурсов, в частности GPU памяти и времени. LoRA работает по принципу, подкрепленному исследованиями, согласно которому изменения, необходимые для адаптации модели, часто находятся в низкоразмерном пространстве, а значит, не требуют изменения каждого отдельного веса.

Вместо того чтобы изменять все исходные веса, LoRA замораживает их и вводит меньшие, поддающиеся обучению матрицы "низкого ранга" в определенные слои архитектуры модели, часто в блоки трансформеров (распространенный компонент многих больших моделей, о котором подробнее рассказывается в статье Attention Is All You Need). Только эти вновь добавленные матрицы (часто называемые адаптерами) обновляются в процессе тонкой настройки. Это значительно сокращает количество обучаемых параметров, часто на порядки (например, миллионы вместо миллиардов), но при этом во многих случаях достигается производительность, сопоставимая с полной тонкой настройкой. В оригинальной научной статье LoRA приводятся дополнительные технические подробности о методологии и ее эффективности. Благодаря такому подходу процесс тонкой настройки происходит значительно быстрее и занимает меньше памяти.

Актуальность и преимущества
Основное преимущество LoRA - это его эффективность, что приводит к нескольким ключевым преимуществам:
- Снижение вычислительных затрат: По сравнению с полной тонкой настройкой требуется значительно меньше памятиGPU и вычислительной мощности, что делает возможным адаптацию больших моделей на менее мощном оборудовании.
- Меньше места для хранения: Поскольку исходные веса модели заморожены, для каждой конкретной задачи нужно сохранять только небольшие адаптеры LoRA. Это гораздо эффективнее, чем хранить полную копию точно настроенной модели для каждой задачи.
- Более быстрое переключение между задачами: загрузка разных адаптеров LoRA позволяет быстро переключаться между задачами без загрузки совершенно новых больших моделей.
- Сравнимая производительность: Несмотря на обучение гораздо меньшего количества параметров, LoRA часто достигает уровня точности, схожего с тем, который достигается при полной тонкой настройке на конкретных задачах.
- Обеспечение возможности пограничного развертывания: Снижение требований к ресурсам облегчает адаптацию моделей для сценариев пограничных вычислений, где вычислительная мощность и память ограничены, принося мощные возможности ИИ на такие устройства, как смартфоны или встраиваемые системы(Edge AI explained by Intel).
- Демократизация: Снижает барьер для исследователей и разработчиков, желающих адаптировать современные модели, такие как GPT-4 или Ultralytics YOLO модели.

---

# Результаты работы и тестирования системы

---

Пример результатов вывода:

Солнце осветило горизонт Утро оборвало мой сладкий сон.
Пролетело быстро время,
И ветер в окно моё стучится.
Пятница – здравствуй, суббота!


Мне с утра в кровать на бок лёг,
Как будто ещё вчера
Пирожки с вишней мне принёс
И сказал: «Отвечай,
Так же не любишь, как любила!»
Я люблю лишь твои губы
И эту сладкую вату!

Люблю тебя, кофе в постель,


Солнце осветило горизонт Утро оборвало мой сладкий сон
В моей душе снова расцвели розы…
От этих цветов мне грустно без мужчины…
Цветы для любви
Как много надо, чтобы найти Любовь и Отчаянную страсть. А взамен, всё лишь букет мимозы, и в этом всё – ничто и пустота,
Когда вокруг столько влюблённости, что, казалось бы, их уже и не существует? А я просто нашёл и полюбил – без любви ничего уже не могу,


# Выводы

---

В ходе выполениня лабораторной работы были получены приемлемые результаты
Лабораторная подтвердил, что GPT-2  может адаптироваться к стилевым текстов песен.
# Использованные источники
- https://developers.sber.ru/help/gigachat-api/large-language-models
- https://www.ultralytics.com/ru/glossary/lora-low-rank-adaptation#:~:text=LoRA%20(Low%2DRank%20Adaptation)%20%2D%20это%20эффективная%20техника%2C,данных%20без%20переобучения%20всей%20модели.
- https://habr.com/ru/articles/791966/
- https://huggingface.co/blog/trl-peft
